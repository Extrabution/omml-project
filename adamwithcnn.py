# -*- coding: utf-8 -*-
"""adamwithCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8xzaAFidZXVY_h6uDEbn7Rp5luvjeao
"""

import copy

import numpy as np

def huber(y):
    if np.abs(y) <= 1:
        return y**2 / 2
    else:
        return np.abs(y) - 1 / 2

def toy_problem(x):
    pi_values = [10**(-i) for i in range(1, 7)]
    Qi_values = []
    for i in range(1, 7):
        Qi_value = np.random.choice([0, 1], size=6, p=[1- pi_values[i-1], pi_values[i-1]])[0]
        Qi_values.append(Qi_value)
    result = 0
    for i in range(6):
        result += (1 - Qi_values[i]) * huber(x[i] - 1) + Qi_values[i] / np.sqrt(pi_values[i]) * huber(x[i] + 1)

    return result, Qi_values

# Example usage

np.random.seed(43)
x_example = np.random.randn(6)  # Replace with your input vector
y_example = 5
result_example, qi = toy_problem(x_example)
print(f"The result for x = {x_example} is: {result_example}")

def derivative(x, Qi_values):
    pi_values = [10**(-i) for i in range(1, 7)]

    gradient = np.zeros_like(x)

    for i in range(6):
        factor = 1 / np.sqrt(pi_values[i])
        if np.abs(x[i] - 1) <= 1:
            grad_huber_minus = x[i] - 1
        else:
            grad_huber_minus = np.sign(x[i] - 1)

        if np.abs(x[i] + 1) <= 1:
            grad_huber_plus = x[i] + 1
        else:
            grad_huber_plus = np.sign(x[i] + 1)

        gradient[i] = (1 - Qi_values[i]) * grad_huber_minus + Qi_values[i] * factor * grad_huber_plus
    return gradient
print(derivative(x_example, qi))

def adam(x, objective, derivative, alpha, beta1, beta2, eps=1e-8, n_iter=100):
    m = [0.0 for _ in range(6)]
    v = [0.0 for _ in range(6)]
    gradients = []
    score, qi = objective(x)
    for t in range(n_iter):
        gradient = derivative(x,qi)
        g = gradient
        for i in range(6):
            m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]
            v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2
            #mhat = m[i] / (1.0 - np.power(beta1, t+1))
            #vhat = v[i] / (1.0 - np.power(beta2, t+1))
            mhat = m[i] / (1.0 - beta1 ** (t+1))
            vhat = v[i] / (1.0 - beta2 ** (t+1))
            x[i] = x[i] - alpha * mhat / (np.sqrt(vhat) + eps)
        score = objective(x)[0]
        gradients.append(g)
        #print((t, x, score, g))
        if t % 10000 == 0:
            print(t, x, score, g)
    return x, score, gradients

np.random.seed(43)
x = copy.deepcopy(x_example)
y_example = 5
_, _, gs = adam(x, toy_problem, derivative, 0.02, 0.8, 0.999, n_iter=10**3)

def expected_gradient_norm_squared(gradients, num_samples=1000): # Generate random samples
    squared_norms = np.zeros(len(gradients))

    for i in range(len(gradients)):
        gradient = gradients[i]
        squared_norms[i] = np.sum(gradient**2)

    expected_value = np.mean(squared_norms)
    return expected_value

ev = expected_gradient_norm_squared(gs)
print(ev)

x = copy.deepcopy(x_example)
alphas = []
for alpha in [10**-i for i in range(4, 2, -1)]:
    _, _, gs = adam(x, toy_problem, derivative, alpha, 0, 1 - 10**-6, n_iter=10**3)
    alphas.append(expected_gradient_norm_squared(gs))
print(alphas)

x = copy.deepcopy(x_example)
b1 = []
for beta1 in [0., 0.5, 0.8, 0.9, 0.99]:
    _, _, gs = adam(x, toy_problem, derivative, 10**-5, beta1, 1 - 10**-6, n_iter=10**3)
    b1.append(expected_gradient_norm_squared(gs))

x = copy.deepcopy(x_example)
b2 = []
for beta2 in [0.9, 0.99, 0.999, 0.9999]:
    _, _, gs = adam(x, toy_problem, derivative, 10**-6, 0, beta2, n_iter=10**3)
    b2.append(expected_gradient_norm_squared(gs))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

figure, axis = plt.subplots(2, 2)
axis[0,0].scatter([10**-4,10**-3], alphas)
# For Sine Function
#axis[0, 0].plot([10**-4,10**-3], alphas)
axis[0, 0].set_title("alpha")

# For Cosine Function
axis[0, 1].plot([0., 0.5, 0.8, 0.9, 0.99], b1)
axis[0, 1].set_title("1 - beta1")

# For Tangent Function
axis[1, 0].plot([0.9, 0.99, 0.999, 0.9999], b2)
axis[1, 0].set_title("1 - beta2")

plt.show()

def plot_variation(parameter_values, param_name, n_iter=10**3, batch_size=1):
    results = []
    for param_value in parameter_values:
        x_t = copy.deepcopy(x_example)
        if param_name == 'alpha':
            alpha = param_value
            beta1 = 0
            beta2 = 1 - 10**-6
        elif param_name == 'beta1':
            alpha = 10**-5
            beta1 = param_value
            beta2 = 1 - 10**-6
        elif param_name == 'beta2':
            alpha = 10**-6
            beta1 = 0
            beta2 = param_value

        squared_norm = 0
        #for _ in range(n_iter):
            #x_t = np.random.randn(6)
        _, _, gradient = adam(x_t, toy_problem, derivative, alpha, beta1, beta2, n_iter=n_iter)
        expected_squared_norm = expected_gradient_norm_squared(gradient,n_iter)

        expected_squared_norm = expected_squared_norm / n_iter
        results.append(expected_squared_norm)

    plt.plot(parameter_values, results, marker='o')
    plt.xscale('log')
    plt.xlabel(param_name)
    plt.ylabel('E [‖∇F (xτ )‖2^2')
    plt.title(param_name)
    plt.show()

n=10**4
# Vary alpha in log scale between 10^(-6) and 1
alpha_values = np.logspace(-6, 0, 13)
plot_variation(alpha_values, 'alpha', n_iter=n)

# Vary 1 - beta1 in log scale between 10^(-6) and 1
beta1_values = np.logspace(-6, 0, 13)
plot_variation(beta1_values, 'beta1', n_iter=n)

# Vary 1 - beta2 in log scale between 10^(-6) and 1
beta2_values = np.logspace(-6, 0, 13)
plot_variation(beta2_values, 'beta2', n_iter=n)

import os
import tarfile
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
from torch import optim
from torchvision.datasets import ImageFolder
from torchvision.datasets.utils import download_url
from torchvision.transforms import ToTensor

dataset_url = "https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz"
download_url(dataset_url, '.')
with tarfile.open('./cifar10.tgz', 'r:gz') as tar:
    tar.extractall(path='./data')
#train_ds = ImageFolder('./data/cifar10/train', transform=ToTensor())
#test_ds = ImageFolder('./data/cifar10/test', transform=ToTensor())

batch_size = 128

#train_loader =DataLoader(train_ds, batch_size, shuffle=True)
#test_loader = DataLoader(test_ds, batch_size*2, shuffle=True)

if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
device

def ConvLayer(inp, out, ks=3, s=1, p=1):
    return nn.Conv2d(inp, out, kernel_size=ks, stride=s, padding=p)

class CIFAR10(nn.Module):
    def __init__(self):
        super().__init__()
        self.neural_net = nn.Sequential(
            ConvLayer(3, 32), nn.ReLU(),
            ConvLayer(32, 64), nn.ReLU(),
            nn.MaxPool2d(2, 2),
            ConvLayer(64, 128), nn.ReLU(),
            ConvLayer(128, 256), nn.ReLU(),
            nn.MaxPool2d(2, 2),
            ConvLayer(256, 512), nn.ReLU(),
            ConvLayer(512, 1024), nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Flatten(),
            nn.Linear(1024*4*4, 1024), nn.ReLU(),
            nn.Linear(1024, 512), nn.ReLU(),
            nn.Linear(512, 10)
        )
    def forward(self, x):
        return self.neural_net(x)

model = CIFAR10().to(device)

def accuracy_score(out, labels):
    _, preds = torch.max(out, dim=1)
    correct_preds = torch.sum(preds==labels).item()
    total_preds = len(preds)
    accuracy = torch.tensor(correct_preds/total_preds)
    return accuracy

import numpy as np

class AdamOptimizer:
    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.alpha = alpha
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None
        self.v = None
        self.t = 0

    def initialize(self, parameters):
        self.m = [np.zeros_like(param.detach().numpy()) for param in parameters]
        self.v = [np.zeros_like(param.detach().numpy()) for param in parameters]
        print(len(self.m[0]))

    def update(self, gradients, parameters):
        if self.m is None:
            self.initialize(parameters)

        self.t += 1
        lr_t = self.alpha * np.sqrt(1 - self.beta2**self.t) / (1 - self.beta1**self.t)

        print(len(parameters))
        for i in range(len(parameters)):
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients[i]
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (gradients[i] ** 2)

            m_hat = self.m[i] / (1 - self.beta1**self.t)
            v_hat = self.v[i] / (1 - self.beta2**self.t)

            parameters[i] -= lr_t * m_hat / (np.sqrt(v_hat) + self.epsilon)


# Create an instance of AdamOptimizer
# Update the parameters using the gradients

from tqdm import tqdm
num_epochs = 1
learning_rate = 0.0003
loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=10**-3, betas=(0.9, 0.999))

for epoch in range(num_epochs):
    total_testing_accuracies = []
    for img, labels in tqdm(train_loader):
        img = img.to(device)
        labels = labels.to(device)
        pred = model(img)
        loss = loss_func(pred, labels)
        loss.backward()
        optimizer.zero_grad()
        optimizer.step()

    for test_img, test_labels in test_loader:
        test_img = test_img.to(device)
        test_labels = test_labels.to(device)
        test_preds = model(test_img)
        metrics = accuracy_score(test_preds, test_labels)
        total_testing_accuracies.append(metrics)
        print("Epoch Number: ", epoch, " | Testing Accuracy Score: ", sum(total_testing_accuracies)/len(total_testing_accuracies))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
from tqdm import tqdm
def plot_variation(parameter_values, param_name, n_iter=10**3, batch_size=1):
    results = []
    loss_func = nn.CrossEntropyLoss()
    for param_value in parameter_values:
        if param_name == 'alpha':
            alpha = param_value
            beta1 = 0
            beta2 = 1 - 10**-6
        elif param_name == 'beta1':
            alpha = 10**-5
            beta1 = param_value
            beta2 = 1 - 10**-6
        elif param_name == 'beta2':
            alpha = 10**-6
            beta1 = 0
            beta2 = param_value

        squared_norm = 0

        train_ds = ImageFolder('./data/cifar10/train', transform=ToTensor())
        test_ds = ImageFolder('./data/cifar10/test', transform=ToTensor())

        batch_size = 128

        train_loader =DataLoader(train_ds, batch_size, shuffle=True)
        test_loader = DataLoader(test_ds, batch_size*2, shuffle=True)

        model = CIFAR10().to(device)
        optimizer = optim.Adam(model.parameters(), lr=alpha, betas=(beta1, beta2))
        gradients = 0
        for _ in range(n_iter):
            #x_t = np.random.randn(6)
            total_testing_accuracies = []
            for img, labels in tqdm(train_loader):
                img = img.to(device)
                labels = labels.to(device)
                pred = model(img)
                loss = loss_func(pred, labels)
                loss.backward(retain_graph=True)
                gr = torch.autograd.grad(loss, model.parameters())
                gr2_mean = torch.mean(torch.sum(torch.square(gr[0]), dim=1))
                gradients += gr2_mean

                optimizer.zero_grad()
                optimizer.step()



            for test_img, test_labels in test_loader:
                test_img = test_img.to(device)
                test_labels = test_labels.to(device)
                test_preds = model(test_img)
                metrics = accuracy_score(test_preds, test_labels)
                total_testing_accuracies.append(metrics)
            print("Epoch Number: ", " | Testing Accuracy Score: ", sum(total_testing_accuracies)/len(total_testing_accuracies))


        expected_squared_norm = gradients / len(train_loader)
        results.append(expected_squared_norm.cpu())

    plt.plot(parameter_values, results, marker='o')
    plt.xscale('log')
    plt.xlabel(param_name)
    plt.ylabel('E [‖∇F (xτ )‖2^2')
    plt.title(param_name)
    plt.show()

n=1
# Vary alpha in log scale between 10^(-6) and 1
alpha_values = np.logspace(-6, 0, 9)
#plot_variation(alpha_values, 'alpha', n_iter=n)

# Vary 1 - beta1 in log scale between 10^(-6) and 1
beta1_values = np.logspace(-6, 0, 9)
beta1_values[-1] = beta1_values[-1] - 10**-6
plot_variation(beta1_values, 'beta1', n_iter=n)

# Vary 1 - beta2 in log scale between 10^(-6) and 1
beta2_values = np.logspace(-6, 0, 9)
beta2_values[-1] = beta2_values[-1] - 10**-6
plot_variation(beta2_values, 'beta2', n_iter=n)



